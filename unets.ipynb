{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "No Ng lecture, but Stanford's deep learning course has an [excellent video lecture on this topic](https://www.youtube.com/watch?v=nDPWywWRIRo).  Please watch 00:08:00 - 00:33:00 or so (or keep watching to the end: it's pretty interesting stuff).  \n",
    "\n",
    "So far, we've been concerned with the problem of object recognition: a mapping from an image to a class (or a probability of membership in each of N disjoint classes).  This is a useful thing as it allows us to determine the subject in an image.  It's also highly reductive, in the sense that we throw away all of the information about where in the image the object in question was located.  We were able to recover this information in some sense through class activation mapping (CAM).  However, it's important to think about the question that class activation mapping is answering: *What was the neural network looking at when it made its decision?*.  Thus, if one was looking at a picture of a ship, the network may have been looking at a flat horizon, or the ocean surface and saying *the only class that shows up on the ocean is a ship, so I'm going to classify this image as a ship.* Another case that I noticed was when classifying deer, the class activation map often indicated that the void space between the deer's legs was a focus of attention.  Or take the following example, in which CAM hones in on a blurry representation of the face of both the horse and the dog: clearly, the other pixels composing the creature were not used as evidence.\n",
    "\n",
    "<img src=cam.png/>\n",
    "\n",
    "An alternative and more explicit question that we can ask is: *what pixels in this image belong to class k?* As it turns out, we can code that question with a neural network.  \n",
    "\n",
    "What should be the inputs and outputs of this neural network?  Of course, the input is the same as for object recognition: usually a 3-band color image.  However, now instead of an array of probabilities for the entire image, we will have *an array of probabilities for each pixel*, thus our output image should be an image with the same width and height as the input, with N bands, where each band contains the probability of class membership.  Note that for the problem that we will address in this section, we will only use two classes (member of target class versus not member).  When this is the case, the output needs to only be one band: probability of membership.  Since these classes are disjoint, we can subtract the probability of membership from one to get the probability of not-membership.  Thus, we will be taking a color image, and outputting for each pixel the probability that it is a member of our target class.  \n",
    "\n",
    "This also defines our needs in terms of training data, and they are somewhat more onerous.  Rather than having pictures and their associated labels, we need pictures and an associated mask, in which each pixel in the member class has been labeled.  Usually this means that someone has gone and drawn a polygon around the interest object in each image.  That's alot of work!  Fortunately, this has mostly been done for us already.  An excellent and freely available data source that contains exactly this kind of labeled data is called the [COCO dataset](http://cocodataset.org/#home).  There is a bit of overhead associated with getting the annotations in a format that we can easily use and extracting the class of interest.  I have done this preprocessing step for you for a very specific subset of images: those containing dogs.  Let's have a look at some of these images and their associated masks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing as kp\n",
    "\n",
    "data_gen_args = dict(\n",
    "rotation_range=20,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "zoom_range=0.2,validation_split=0.2,horizontal_flip=True)\n",
    "image_datagen = kp.image.ImageDataGenerator(preprocessing_function=lambda x: x/255.,**data_gen_args)\n",
    "mask_datagen = kp.image.ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    './imgs_train',\n",
    "    class_mode=None,\n",
    "    seed=0,target_size=(240,320),batch_size=8)\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    './masks_train',\n",
    "    class_mode=None,\n",
    "    seed=0,target_size=(240,320),color_mode='grayscale',batch_size=8)\n",
    "\n",
    "image_generator_test = image_datagen.flow_from_directory(\n",
    "    './imgs_test',\n",
    "    class_mode=None,\n",
    "    seed=0,target_size=(240,320),batch_size=8)\n",
    "\n",
    "mask_generator_test = mask_datagen.flow_from_directory(\n",
    "    './masks_test',\n",
    "    class_mode=None,\n",
    "    seed=0,target_size=(240,320),color_mode='grayscale',batch_size=8)\n",
    "\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "test_generator = zip(image_generator_test,mask_generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_batch,mask_batch = next(train_generator)\n",
    "fig,axs = plt.subplots(nrows=2,ncols=4,figsize=(12,6))\n",
    "counter = 0\n",
    "for r in axs:\n",
    "    for ax in r:\n",
    "        ax.imshow(img_batch[counter])\n",
    "        ax.imshow(mask_batch[counter,:,:,0],alpha=0.3)\n",
    "        counter += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look all the adorable doggos!  Let's see if we can train a network that can reproduce these masks.  \n",
    "\n",
    "Obviously, we're going to need to come up with some different ideas than what we've seen so far, because continously downscaling the image isn't going to work: the output needs to be the same size as the input.  However, we'll need to do some downscaling, both for the sake of computational efficiency and to increase the size of our network's perceptive field (the parts of the image that go into classifying a particular pixel).  A network architecture that has proven extremely useful for this task is called [U-net](https://arxiv.org/pdf/1505.04597.pdf), so called because it resembles a U.  It borrows some ideas from ResNet as well in that there are skip connections, although these are a bit different than the ResNet ones.  U-net looks like this:\n",
    "<img src=unet.svg width=800/>\n",
    "Note that to save space, when the convolution kernel size is not specified, it's 3x3.  What does this network do?  Essentially, what we have is a sequence of image downscalings, followed by a sequence of upscalings to get us back to the original image size.  Without the skip connections, using this architecture for the problem we're considering does work, however the resulting segmentation tends to be very \"low-res\" for lack of a better word: edges aren't captured well and spatial localization is poor.  Why is this?  During the downsampling step, we're throwing away information, and as we are well aware, it isn't possible to recover the original image after downscaling.  U-net solves this problem by concatenating the output of each downsampling layer with the commensurately sized upsampling layer.  Thus, each layer gets access to the features that were generated during downsampling at its own scale *along with the larger scale information produced by further downsampling*.\n",
    "\n",
    "Note that the figure above shows a U-net that downsamples twice (and upsamples twice).  However, in practice deeper networks are often used.  The following code gives a U-net that downsamples and upsamples only once.  **Extend the network below such that it has a total of four downsampling steps and four upsampling steps.**   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as kl\n",
    "import keras.models as km\n",
    "\n",
    "def unet(input_size):\n",
    "    inputs = kl.Input(input_size)\n",
    "    conv1 = kl.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = kl.BatchNormalization()(conv1)\n",
    "    conv1 = kl.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = kl.BatchNormalization()(conv1)\n",
    "    pool1 = kl.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "     \n",
    "    conv5 = kl.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv5 = kl.BatchNormalization()(conv5)\n",
    "    conv5 = kl.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = kl.BatchNormalization()(conv5)\n",
    "\n",
    "    depool9 = kl.UpSampling2D(size = (2,2))(conv5)\n",
    "    upconv9 = kl.Conv2D(64, 2, activation = 'linear', padding = 'same', kernel_initializer = 'he_normal')(depool9)\n",
    "    merge9 = kl.concatenate([conv1,upconv9], axis = 3)\n",
    "    conv9 = kl.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = kl.BatchNormalization()(conv9)\n",
    "    conv9 = kl.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = kl.BatchNormalization()(conv9)\n",
    "    conv10 = kl.Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = km.Model(input = inputs, output = conv10)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = unet((240,320,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.optimizers as ko\n",
    "model.compile(optimizer = ko.Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks as kc\n",
    "filepath = './checkpoints'\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = kc.ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-4\n",
    "    if epoch > 40:\n",
    "        lr /= 8.\n",
    "    elif epoch > 25:\n",
    "        lr /= 4.\n",
    "    elif epoch > 10:\n",
    "        lr /= 2.\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = kc.LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=(3041-305)//8,epochs=10,validation_data=test_generator,validation_steps=301//8,callbacks=[checkpoint,lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction on the test set\n",
    "\n",
    "img = image_generator_test.next()\n",
    "mask_true = mask_generator_test.next()\n",
    "mask = model.predict(img)\n",
    "plt.imshow(img[0].squeeze())\n",
    "plt.imshow(mask[0].squeeze(),alpha=0.3,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
